{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce65d5c2",
   "metadata": {},
   "source": [
    "# Week 5: Tractable Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf538a0",
   "metadata": {},
   "source": [
    "### Author: Rajit Rajpal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedcef7",
   "metadata": {},
   "source": [
    "The goal of this tutorial will be to learn about tractable generative models (e.g Autoregressive, Normalizing Flows) that give exact likelihoods to perform tasks like out-of-distribution (OOD) detection. `# TODO` are for you to implement. Solutions will contain the completed code.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5516dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy matplotlib torch torchvision scipy pillow scikit-learn <- Uncomment and run this if you do not have the necessary packages.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfae021",
   "metadata": {},
   "source": [
    "## 1 Autoregressive Model\n",
    "\n",
    "The core idea of an autoregressive model is to decompose the joint distribution of a\n",
    "$D$-dimensional vector $x$ into a product of conditional distributions using the\n",
    "chain rule of probability:\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{i=1}^{D} p(x_i \\mid x_{<i})\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "x_{<i} = [x_1, \\dots, x_{i-1}].\n",
    "$$\n",
    "\n",
    "We will be using the binarized MNIST dataset, the same as last time except each pixel is either 0 (black) or 1 (white). This is done in order to facilitate quick training.\n",
    "For binarized MNIST, each $x_i$ is a Bernoulli random variable. The model outputs the\n",
    "probability\n",
    "$$\n",
    "\\theta_i = p(x_i = 1 \\mid x_{<i}).\n",
    "$$\n",
    "\n",
    "### Autoregressive Constraint\n",
    "\n",
    "To compute this in a single forward pass, we use MADE. We apply *masks* to the weight\n",
    "matrices of a standard MLP to ensure that a hidden unit $h_j$ only depends on a subset\n",
    "of inputs, and the output $\\hat{x}_i$ depends only on\n",
    "$x_1, \\dots, x_{i-1}$.\n",
    "\n",
    "### Loss Function (Negative Log-Likelihood)\n",
    "\n",
    "For binarized data, we minimize the binary cross-entropy:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) =\n",
    "- \\sum_{i=1}^{D}\n",
    "\\left[\n",
    "x_i \\log \\hat{x}_i + (1 - x_i)\\log(1 - \\hat{x}_i)\n",
    "\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0245631",
   "metadata": {},
   "source": [
    "### 1.1 Architecture\n",
    "\n",
    "We need a custom Linear layer that can zero out specific connections to enforce the autoregressive property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4342c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" A linear layer with a configurable mask to enforce autoregressive constraints \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype('float32')))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0f59d",
   "metadata": {},
   "source": [
    "We use a model called MADE (masked autoregressive density estimator). We won't go too much in detail regarding the implementation of this architecture. The basic idea is:\n",
    "\n",
    "A standard autoencoder takes an input vector $x$ and predicts a reconstruction $\\hat{x}$. However, in a fully connected architecture, the predicted component $\\hat{x}_i$ may depend on $x_i$ itself (or even on future variables $(x_{i+1}, \\dots)$), which violates the autoregressive requirement needed to model $p(x_i \\mid x_{<i})$.\n",
    "\n",
    "MADE enforces autoregressive structure by applying a binary mask $M$ to the weight matrix $W$ of each layer, restricting which connections are allowed. The resulting transformation is\n",
    "$$\n",
    "h = \\text{activation}\\left((W \\odot M)x + b\\right),\n",
    "$$\n",
    "where the mask ensures that each output dimension depends only on the appropriate subset of inputs. By carefully constructing these masks, MADE guarantees that the network represents a valid autoregressive model while retaining the efficiency of a single forward pass.\n",
    "\n",
    "Please refer to this paper (https://arxiv.org/abs/1502.03509) for more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23130c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.net = nn.Sequential()\n",
    "        \n",
    "        # Build the network\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            m_layer = MaskedLinear(dims[i], dims[i+1])\n",
    "            self.net.add_module(f\"layer_{i}\", m_layer)\n",
    "            self.layers.append(m_layer)\n",
    "            if i < len(dims)-2:\n",
    "                self.net.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "        \n",
    "        self.m = {}\n",
    "        self.create_masks()\n",
    "\n",
    "    def create_masks(self):\n",
    "        L = len(self.layers)\n",
    "        # Assign degrees to input nodes: 1 to D\n",
    "        self.m[-1] = np.arange(1, self.input_dim + 1)\n",
    "        \n",
    "        # Degrees for hidden layers: random integers between 1 and D-1\n",
    "        for l in range(L - 1):\n",
    "            self.m[l] = np.random.randint(1, self.input_dim, size=self.layers[l].out_features)\n",
    "            \n",
    "        # The output layer degrees must match the input degrees for the auto-regressive property\n",
    "        self.m[L-1] = np.arange(1, self.input_dim + 1)\n",
    "        \n",
    "        # Apply masks to the MaskedLinear layers\n",
    "        for l in range(L):\n",
    "            # Mask logic: hidden unit j can only see input units i where m(i) <= m(j)\n",
    "            # For the output layer, it's a strict inequality: m(i) < m(j)\n",
    "            if l < L - 1:\n",
    "                mask = self.m[l-1][None, :] <= self.m[l][:, None]\n",
    "            else:\n",
    "                mask = self.m[l-1][None, :] < self.m[l][:, None]\n",
    "            \n",
    "            self.layers[l].set_mask(mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ac814",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading\n",
    "We will load MNIST and apply a threshold to binarize the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x > 0.5).float().view(-1)) # Binarize and Flatten\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=128)\n",
    "\n",
    "# Initialize Model\n",
    "# Constants\n",
    "INPUT_DIM = 784\n",
    "HIDDEN_DIMS = [512, 512]\n",
    "\n",
    "# Initialize Model\n",
    "model = MADE(INPUT_DIM, HIDDEN_DIMS, INPUT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Model initialized on {device}. Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfd499",
   "metadata": {},
   "source": [
    "### 1.3 Training\n",
    "We will train by minimizing negative log-likelihood (NLL). We compute it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nll(data_loader):\n",
    "    model.eval()\n",
    "    total_nll = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            # TODO: Binary Cross Entropy is the NLL for Bernoulli variables\n",
    "            loss = F.binary_cross_entropy_with_logits(..., ..., reduction='sum')\n",
    "            total_nll += loss.item()\n",
    "    return total_nll / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(1, 25): # This should take roughly 4 minutes\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # TODO\n",
    "        loss = F.binary_cross_entropy_with_logits(..., ..., reduction='sum') / data.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    test_nll = compute_nll(test_loader)\n",
    "    print(f\"Epoch {epoch} | Test NLL: {test_nll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a5912",
   "metadata": {},
   "source": [
    "### 1.4 Inference\n",
    "We will generate and visualize some samples, and also compute the NLL for them. \n",
    "The computation of likelihood in MADE is based on the product rule of probability. Specifically, the model assumes that the joint distribution over the input dimensions can be factorized as a sequence of conditional probabilities:\n",
    "$$\n",
    "p(x) = p(x_1)\\, p(x_2 \\mid x_1)\\, \\cdots\\, p(x_D \\mid x_{1:D-1}).\n",
    "$$\n",
    "\n",
    "Due to the autoregressive masking constraints, a single forward pass through the network produces the parameters (e.g., Bernoulli logits or Gaussian means and variances) for all $D$ conditional distributions simultaneously. The exact likelihood of an input $x$ is then obtained by taking the product of these individual conditional probabilities.\n",
    "\n",
    "This likelihood is *exact* because no stochastic approximation is involved. Unlike variational autoencoders, which rely on an evidence lower bound (ELBO), MADE directly evaluates the density defined by the chain rule of probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, num_samples=16):\n",
    "    model.eval()\n",
    "    # Initialize samples with zeros\n",
    "    samples = torch.zeros(num_samples, 784).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # We must iterate through each pixel sequentially\n",
    "        for i in range(784):\n",
    "            logits = model(samples)\n",
    "            # TODO: Get the probability for the i-th pixel\n",
    "            probs = torch.sigmoid(...)\n",
    "            # Sample from Bernoulli distribution\n",
    "            samples[:, i] = torch.bernoulli(probs)\n",
    "    \n",
    "    # Reshape for plotting\n",
    "    samples = samples.view(-1, 28, 28).cpu().numpy()\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\"Samples Generated by MADE\")\n",
    "    plt.show()\n",
    "\n",
    "generate_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f7928",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "What do you observe about these samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac211",
   "metadata": {},
   "source": [
    "### 1.5 Out-of-Distribution (OOD) detection\n",
    "We will try to perform OOD detection against FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Datasets and Loaders (Removed EMNIST due to download issues)\n",
    "mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "fashion_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "fashion_loader = DataLoader(fashion_test, batch_size=128, shuffle=False)\n",
    "\n",
    "def get_log_likelihoods(model, data_loader):\n",
    "    model.eval()\n",
    "    log_probs = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            # TODO: Log-likelihood for Bernoulli: -BCE\n",
    "            # reduction='none' allows us to sum over the ___ dimension\n",
    "            # while keeping the batch dimension intact.\n",
    "            lp = -F.binary_cross_entropy_with_logits(logits, data, reduction='none').sum(dim=...)\n",
    "            log_probs.extend(lp.cpu().numpy())\n",
    "    return np.array(log_probs)\n",
    "\n",
    "# 2. Compute Likelihoods\n",
    "mnist_ll = get_log_likelihoods(model, test_loader)\n",
    "fashion_ll = get_log_likelihoods(model, fashion_loader)\n",
    "\n",
    "# 3. Plot Histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Determine a sensible range (ignoring extreme outliers)\n",
    "# We look at the 1st and 99th percentiles to focus on the main distribution\n",
    "combined_ll = np.concatenate([mnist_ll, fashion_ll])\n",
    "min_range = np.percentile(combined_ll, 1)\n",
    "max_range = np.percentile(combined_ll, 99)\n",
    "\n",
    "plt.hist(mnist_ll, bins=60, range=(min_range, max_range), alpha=0.6, \n",
    "         label='In-Distribution (MNIST)', color='royalblue', density=True)\n",
    "plt.hist(fashion_ll, bins=60, range=(min_range, max_range), alpha=0.6, \n",
    "         label='OOD (FashionMNIST)', color='forestgreen', density=True)\n",
    "\n",
    "plt.title(\"OOD Detection: Log-Likelihood Distributions\")\n",
    "plt.xlabel(\"Log-Likelihood (Higher is more 'likely')\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.xlim(min_range, max_range)\n",
    "plt.show()\n",
    "\n",
    "# 4. Visual Comparison Grid\n",
    "def plot_multi_comparison(datasets_list, ll_list, names, n=5):\n",
    "    fig, axes = plt.subplots(len(datasets_list), n, figsize=(15, 3 * len(datasets_list)))\n",
    "    for row_idx, (dataset, lls, name) in enumerate(zip(datasets_list, ll_list, names)):\n",
    "        for col_idx in range(n):\n",
    "            img, _ = dataset[col_idx]\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            ax.imshow(img.view(28, 28), cmap='gray')\n",
    "            ax.set_title(f\"{name} LL: {lls[col_idx]:.1f}\", fontsize=9)\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first 5 samples from each\n",
    "plot_multi_comparison(\n",
    "    [mnist_test, fashion_test], \n",
    "    [mnist_ll, fashion_ll], \n",
    "    [\"MNIST\", \"Fashion\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa51ec",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "\n",
    "Compare and contrast the samples from MNIST and FashionMNIST. Why do you think the log-likelihoods are so different? Is it justified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89e3ff",
   "metadata": {},
   "source": [
    "# 2 Normalizing Flows\n",
    "\n",
    "Another interesting class of tractable generative models are normalizing flows. While MADE uses masking to enforce the chain rule, Normalizing Flows use invertible transformations and the Change of Variables formula to map a simple distribution to a complex one.\n",
    "\n",
    "\n",
    "The core of a Normalizing Flow is a sequence of invertible functions\n",
    "$$\n",
    "f = f_K \\circ \\cdots \\circ f_1.\n",
    "$$\n",
    "Given a base distribution $z \\sim p_z(z)$ (typically a standard Gaussian), we transform it via $x = f(z)$. Using the change of variables formula, the exact log-likelihood of $x$ is\n",
    "$$\n",
    "\\log p_x(x) = \\log p_z\\!\\left(f^{-1}(x)\\right)\n",
    "+ \\log \\left| \\det \\left( \\frac{\\partial x}{\\partial f^{-1}} \\right) \\right|.\n",
    "$$\n",
    "\n",
    "To make likelihood computation efficient, the Jacobian\n",
    "$$\n",
    "\\frac{\\partial x}{\\partial f^{-1}}\n",
    "$$\n",
    "must have a determinant that is easy to compute.\n",
    "\n",
    "**RealNVP** achieves this by using affine coupling layers whose Jacobian matrices are triangular. The input vector $x$ is split into two parts:\n",
    "$$\n",
    "x = [x_{1:d},\\; x_{d+1:D}].\n",
    "$$\n",
    "\n",
    "The first part is left unchanged:\n",
    "$$\n",
    "y_{1:d} = x_{1:d}.\n",
    "$$\n",
    "\n",
    "The second part is scaled and shifted conditioned on the first:\n",
    "$$\n",
    "y_{d+1:D} = x_{d+1:D} \\odot \\exp\\bigl(s(x_{1:d})\\bigr) + t(x_{1:d}),\n",
    "$$\n",
    "where $s(\\cdot)$ and $t(\\cdot)$ are neural networks.\n",
    "\n",
    "The log-determinant of the Jacobian for this transformation is simply\n",
    "$$\n",
    "\\sum s(x_{1:d}).\n",
    "$$\n",
    "Because the functions $s$ and $t$ do not need to be invertible, they can be arbitrarily expressive, allowing RealNVP to learn highly complex distributions.\n",
    "\n",
    "### 2.1 Affine Coupling Layer\n",
    "\n",
    "We will implement the ```AffineCoupling``` layer and the ```RealNVP``` wrapper. For binarized MNIST, we use Dequantization by adding Uniform noise $u \\in [0,1)$ to the binary pixels to make the discrete data continuous, which is required for flows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, mask):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        # s and t are arbitrary neural networks\n",
    "        self.s_net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim), nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, dim), nn.Tanh()\n",
    "        )\n",
    "        self.t_net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim), nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        # Apply mask to keep part of the input constant\n",
    "        x_masked = x * self.mask\n",
    "        s = self.s_net(x_masked) * (1 - self.mask)\n",
    "        t = self.t_net(x_masked) * (1 - self.mask)\n",
    "\n",
    "        if not reverse: # z -> x (mapping prior to data)\n",
    "            y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "            log_det = s.sum(dim=1)\n",
    "            return y, log_det\n",
    "        else: # x -> z (mapping data to prior)\n",
    "            y = x_masked + (1 - self.mask) * ((x - t) * torch.exp(-s))\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e5156",
   "metadata": {},
   "source": [
    "### 2.2 RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.dim = dim # Store the dimension\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Alternate masks (010101... and 101010...)\n",
    "        mask = torch.arange(dim).float() % 2\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(AffineCoupling(dim, hidden_dim, mask))\n",
    "            mask = 1 - mask\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # Only dequantize if we are doing high-dim image data (optional check)\n",
    "        if self.dim > 2:\n",
    "            x = x + torch.rand_like(x)\n",
    "        \n",
    "        log_det_sum = 0\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer(x)\n",
    "            log_det_sum += log_det\n",
    "            \n",
    "        prior = torch.distributions.Normal(0, 1)\n",
    "        log_prior = prior.log_prob(x).sum(dim=1)\n",
    "        \n",
    "        return ... # TODO: Implement this\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        # Use self.dim instead of 784\n",
    "        z = torch.randn(num_samples, self.dim).to(device)\n",
    "        for layer in reversed(self.layers):\n",
    "            z = layer(z, reverse=True)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bec7b",
   "metadata": {},
   "source": [
    "### 2.3 Training\n",
    "Unlike MADE, which uses Cross-Entropy, RealNVP is trained by directly maximizing the ```log_prob``` output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add43bf",
   "metadata": {},
   "source": [
    "### 2.3.1 2D training\n",
    "\n",
    "Before we begin training on MNIST, let us get some geometric intuition by studying the invertible transformations learnt on cats. We created these data samples by blurring an image of the sketch of a cat in ```cat.png``` and then using rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def get_cat_samples(n_samples=10000, sigma=2.0):\n",
    "    try:\n",
    "        # Load image and convert to grayscale\n",
    "        cat = PIL.Image.open(\"cat.png\").convert('L')\n",
    "        arr = 1.0 - (np.array(cat) / 255.0) # Invert so cat is high density (1)\n",
    "        \n",
    "        # Apply Gaussian Blur to smooth the distribution\n",
    "        smoothed_arr = gaussian_filter(arr, sigma=sigma)\n",
    "        # Normalize to make it a valid probability map\n",
    "        prob_map = smoothed_arr / smoothed_arr.sum()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cat image not found, using a fallback density.\")\n",
    "        # Fallback to a simple 2D Gaussian if file is missing\n",
    "        return torch.randn(n_samples, 2) * 0.5\n",
    "\n",
    "    # Rejection Sampling or Weighted Sampling\n",
    "    flat_probs = prob_map.flatten()\n",
    "    indices = np.random.choice(len(flat_probs), size=n_samples, p=flat_probs)\n",
    "    \n",
    "    # Convert 1D indices back to 2D coordinates\n",
    "    rows, cols = np.unravel_index(indices, prob_map.shape)\n",
    "    \n",
    "    # Map to [-1, 1] range\n",
    "    points = np.zeros((n_samples, 2))\n",
    "    points[:, 0] = (cols / prob_map.shape[1]) * 2 - 1\n",
    "    points[:, 1] = 1 - (rows / prob_map.shape[0]) * 2\n",
    "    \n",
    "    # Add a tiny bit of noise to prevent points from collapsing onto pixel grid\n",
    "    points += np.random.normal(0, 0.01, points.shape)\n",
    "    \n",
    "    return torch.from_numpy(points).float()\n",
    "\n",
    "# Generate blurred training data\n",
    "# Try increasing sigma to 3 or 5 for even more \"fuzziness\"\n",
    "cat_points = get_cat_samples(10000, sigma=3.0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(cat_points[:, 0], cat_points[:, 1], s=1, alpha=0.2, color='purple')\n",
    "plt.title(\"Smoothed Target Distribution (Blurred Cat)\")\n",
    "plt.xlim(-1.2, 1.2); plt.ylim(-1.2, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller 2D model\n",
    "# Re-run training with the blurred data\n",
    "cat_flow = RealNVP(dim=2, hidden_dim=128, n_layers=8).to(device)\n",
    "optimizer = torch.optim.Adam(cat_flow.parameters(), lr=2e-4)\n",
    "\n",
    "for i in range(2001):\n",
    "    cat_flow.train()\n",
    "    # Sample fresh points every iteration to avoid overfitting to a static set\n",
    "    x = get_cat_samples(1024, sigma=3.0).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = ... # TODO: Implement negative log-likelihood using attributes from the cat_flow class\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"Iteration {i} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualization of intermediate transformations\n",
    "def visualize_dual_warps(model, cat_data, num_samples=2000):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Setup for Forward (Cat -> Gaussian)\n",
    "    fwd_x = cat_data[:num_samples].to(device)\n",
    "    \n",
    "    # Setup for Reverse (Gaussian -> Cat)\n",
    "    rev_z = torch.randn(num_samples, 2).to(device)\n",
    "    \n",
    "    # Setup Grid\n",
    "    grid_size = 20\n",
    "    x_range = np.linspace(-2, 2, grid_size)\n",
    "    y_range = np.linspace(-2, 2, grid_size)\n",
    "    grid_x, grid_y = np.meshgrid(x_range, y_range)\n",
    "    grid_tensor = torch.from_numpy(np.stack([grid_x, grid_y], axis=-1)).float().view(-1, 2).to(device)\n",
    "    \n",
    "    layers = model.layers\n",
    "    num_layers = len(layers)\n",
    "    fig, axes = plt.subplots(2, num_layers + 1, figsize=(3 * (num_layers + 1), 6))\n",
    "    \n",
    "    # Trackers\n",
    "    curr_fwd_x = fwd_x\n",
    "    curr_fwd_grid = grid_tensor\n",
    "    \n",
    "    curr_rev_z = rev_z\n",
    "    curr_rev_grid = grid_tensor\n",
    "    \n",
    "    # Loop through layers\n",
    "    for i in range(num_layers + 1):\n",
    "        # --- ROW 1: FORWARD (Cat to Gaussian) ---\n",
    "        ax_fwd = axes[0, i]\n",
    "        pts_fwd = curr_fwd_x.cpu().detach().numpy()\n",
    "        grid_fwd = curr_fwd_grid.cpu().detach().numpy().reshape(grid_size, grid_size, 2)\n",
    "        \n",
    "        ax_fwd.scatter(pts_fwd[:, 0], pts_fwd[:, 1], s=1, alpha=0.4, color='blue')\n",
    "        for j in range(grid_size):\n",
    "            ax_fwd.plot(grid_fwd[j, :, 0], grid_fwd[j, :, 1], 'r', alpha=0.1, lw=0.5)\n",
    "            ax_fwd.plot(grid_fwd[:, j, 0], grid_fwd[:, j, 1], 'r', alpha=0.1, lw=0.5)\n",
    "        \n",
    "        ax_fwd.set_xlim(-2, 2); ax_fwd.set_ylim(-2, 2)\n",
    "        ax_fwd.set_title(f\"Fwd Layer {i}\") if i > 0 else ax_fwd.set_title(\"Input (Cat)\")\n",
    "        ax_fwd.axis('off')\n",
    "\n",
    "        # --- ROW 2: REVERSE (Gaussian to Cat) ---\n",
    "        # Note: We visualize the reverse path starting from the last column\n",
    "        ax_rev = axes[1, num_layers - i]\n",
    "        pts_rev = curr_rev_z.cpu().detach().numpy()\n",
    "        grid_rev = curr_rev_grid.cpu().detach().numpy().reshape(grid_size, grid_size, 2)\n",
    "        \n",
    "        ax_rev.scatter(pts_rev[:, 0], pts_rev[:, 1], s=1, alpha=0.4, color='purple')\n",
    "        for j in range(grid_size):\n",
    "            ax_rev.plot(grid_rev[j, :, 0], grid_rev[j, :, 1], 'g', alpha=0.1, lw=0.5)\n",
    "            ax_rev.plot(grid_rev[:, j, 0], grid_rev[:, j, 1], 'g', alpha=0.1, lw=0.5)\n",
    "            \n",
    "        ax_rev.set_xlim(-2, 2); ax_rev.set_ylim(-2, 2)\n",
    "        ax_rev.set_title(f\"Rev Layer {num_layers-i}\") if i < num_layers else ax_rev.set_title(\"Input (Gaussian)\")\n",
    "        ax_rev.axis('off')\n",
    "\n",
    "        # Step forward and backward\n",
    "        if i < num_layers:\n",
    "            with torch.no_grad():\n",
    "                # Forward step\n",
    "                curr_fwd_x, _ = layers[i](curr_fwd_x)\n",
    "                curr_fwd_grid, _ = layers[i](curr_fwd_grid)\n",
    "                \n",
    "                # Reverse step (starting from the back)\n",
    "                rev_layer = layers[num_layers - 1 - i]\n",
    "                curr_rev_z = rev_layer(curr_rev_z, reverse=True)\n",
    "                curr_rev_grid = rev_layer(curr_rev_grid, reverse=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Forward Flow: Analysis (Top) | Reverse Flow: Synthesis (Bottom)\", fontsize=16, y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the dual transformation\n",
    "visualize_dual_warps(cat_flow, get_cat_samples(3000, sigma=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea14859",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "\n",
    "Describe the set of transformations. Do the transformations in reverse and forward match? Why or why not? Why do you think we needed to blur the original image before generating data samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb34c0",
   "metadata": {},
   "source": [
    "### 2.3.3 MNIST training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "flow_model = RealNVP(dim=784, hidden_dim=256, n_layers=8).to(device)\n",
    "flow_optimizer = torch.optim.Adam(flow_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "def train_flow(epochs=5):\n",
    "    flow_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            flow_optimizer.zero_grad()\n",
    "            # TODO: Loss is negative log-likelihood. Dont forget to take mean!\n",
    "            loss = ...\n",
    "            loss.backward()\n",
    "            flow_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "train_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5525e",
   "metadata": {},
   "source": [
    "### 2.4 Inference\n",
    "Generating samples from a Normalizing Flow is extremely fast compared to MADE because it is a single forward pass.\n",
    "\n",
    "For RealNVP, a type of normalizing flow, likelihood computation is based on the change of variables formula. The model assumes that the observed data are generated by transforming samples from a simple latent distribution—typically a standard Gaussian—through an invertible function $f$ that warps this simple distribution into the complex structure of the data (e.g., MNIST images).\n",
    "\n",
    "To evaluate the likelihood of a given data point $x$, we apply the inverse transformation and map the data back into latent space:\n",
    "$$\n",
    "z = f^{-1}(x).\n",
    "$$\n",
    "The exact likelihood is then given by the density of $z$ under the base Gaussian distribution, multiplied by a correction term given by the determinant of the Jacobian of the inverse transformation:\n",
    "$$\n",
    "\\log p_x(x) = \\log p_z(z) + \\log \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right|.\n",
    "$$\n",
    "\n",
    "This Jacobian determinant accounts for how volumes are expanded or contracted by the transformation, ensuring that probability mass is preserved and that the resulting density integrates to 1. Because the transformation is invertible and the Jacobian determinant is computed exactly, RealNVP provides an exact likelihood rather than an approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flow_results(model, test_dataset):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 1. Generate Samples (The Model's Imagination)\n",
    "    with torch.no_grad():\n",
    "        gen_samples = model.sample(16).cpu().view(-1, 28, 28)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    plt.suptitle(\"RealNVP: Generated Samples (Top) vs Real Test Images with LL (Bottom)\", fontsize=16)\n",
    "    \n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(gen_samples[i], cmap='gray')\n",
    "        axes[0, i].set_title(\"Generated\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "    # 2. Evaluate Real Test Images (Likelihood Computation)\n",
    "    # We take 4 random images from the test set\n",
    "    indices = torch.randint(0, len(test_dataset), (4,))\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            x, _ = test_dataset[idx]\n",
    "            x_input = x.unsqueeze(0).to(device)\n",
    "            \n",
    "            # TODO: Compute Exact Log-Likelihood using attributes from the class that defines model\n",
    "            log_prob = ...\n",
    "            \n",
    "            axes[1, i].imshow(x.view(28, 28), cmap='gray')\n",
    "            axes[1, i].set_title(f\"Real LL: {log_prob:.2f}\")\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_flow_results(flow_model, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498384f6",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Why are the samples so blurry and so much worse than MADE? How can we fix it? Do you notice any advantage of RealNVP over MADE, perhaps with the speed? Explain why the speeds of MADE and Normalizing Flows would be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457579b",
   "metadata": {},
   "source": [
    "### 2.5 OOD detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95933ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_log_likelihoods(model, data_loader):\n",
    "    model.eval()\n",
    "    log_probs = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for x, _ in data_loader:\n",
    "            x = x.to(device)\n",
    "            lp = model.log_prob(x)\n",
    "            log_probs.extend(lp.cpu().numpy())\n",
    "    return np.array(log_probs)\n",
    "\n",
    "# Compute Likelihoods for Flow\n",
    "flow_mnist_ll = get_flow_log_likelihoods(flow_model, test_loader)\n",
    "flow_fashion_ll = get_flow_log_likelihoods(flow_model, fashion_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(flow_mnist_ll, bins=50, alpha=0.5, label='MNIST (In)', color='blue', density=True)\n",
    "plt.hist(flow_fashion_ll, bins=50, alpha=0.5, label='Fashion (OOD)', color='green', density=True)\n",
    "plt.title(\"OOD Detection using RealNVP (Normalizing Flow)\")\n",
    "plt.xlabel(\"Exact Log-Likelihood\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fde873",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "Why is the OOD detection still reasonable despite the poor samples generated by the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b8000",
   "metadata": {},
   "source": [
    "### Masked Autoregressive Flows (MAFs): Combining MADE and Normalizing Flows\n",
    "\n",
    "While RealNVP worked beautifully for the 2D cat, it struggles with images for two main reasons: Pixel Dependency and Volume Preservation. RealNVP is \"volume-preserving\" in a way that often leads to \"whitening\" the data. Without enough layers or the right architecture (like multi-scale architectures or convolutions), it treats the 784 pixels as independent values rather than a cohesive structure.\n",
    "\n",
    "*Why RealNVP failed?*\n",
    "\n",
    "- A standard affine coupling layer (splitting the image 50/50) is very \"blind.\" It tries to predict the bottom half of the image based on the top half using a simple MLP. Since MLP-based flows don't have a concept of spatial locality (like Convolutional layers do), they struggle to learn the sharp edges of digits.\n",
    "- RealNVP is \"volume-preserving\" in a way that often leads to \"whitening\" the data. Without enough layers or the right architecture (like multi-scale architectures or convolutions), it treats the 784 pixels as independent values rather than a cohesive structure.\n",
    "- We are adding uniform noise to binary pixels. If the model isn't powerful enough, it spends all its capacity modeling that noise rather than the underlying digit structure.\n",
    "\n",
    "*How to fix it?*\n",
    "\n",
    "If we want the stability of MADE with the power of Normalizing Flows, we can combine them. In a Masked Autoregressive Flow, each layer of the flow is actually a MADE network. This gives the model a much stronger \"inductive bias\" for sequential pixel dependencies. You will notice that MAF gets much better Log-Likelihoods and sharper OOD detection, but sampling becomes slow again (just like the original MADE). \n",
    "\n",
    "This is the fundamental tension in flows:\n",
    "\n",
    "RealNVP: Fast sampling, but harder to train for complex densities.\n",
    "\n",
    "MAF: Easy to train (exact density), but slow sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
