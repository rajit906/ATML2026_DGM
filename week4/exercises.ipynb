{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238ba293",
   "metadata": {},
   "source": [
    "# Week 4 Tutorial (Exercises) - Latent Variable Models and Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92663b7",
   "metadata": {},
   "source": [
    "### Author: Rajit Rajpal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b28179",
   "metadata": {},
   "source": [
    "The goal of this tutorial will be to train various autoencoders (Plain, VAE, IW-VAE) on various datasets (Syntethic, MNIST). `# TODO` are for you to implement. Solutions will contain the completed code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8e041",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "We begin by defining the core components. As shown in the slides, an autoencoder trains a neural network $f_{\\theta, \\phi}$ to predict $x$ from itself through a bottleneck $z$.\n",
    "\n",
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# %pip install numpy matplotlib torch torchvision scipy pillow scikit-learn <- Uncomment and run this if you do not have the necessary packages.\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d60b6",
   "metadata": {},
   "source": [
    "## 1.2 Architecture\n",
    "\n",
    "We will have our autoencoder $f_{\\theta, \\phi}$ be a very simple MLP. Note that the autoencoder has two components: Encoder $E_{\\phi}$ and Decoder $D_{\\theta}$. In order to keep the rest of the code self-contained, we include VAE within the architecture which can be enabled by setting `is_vae=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, is_vae=False):\n",
    "        super().__init__()\n",
    "        self.is_vae = is_vae\n",
    "        # Encoder: compresses high-dim data\n",
    "        self.enc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, latent_dim if not is_vae else latent_dim * 2)\n",
    "        )\n",
    "        # Decoder: reconstructs data from latents\n",
    "        self.dec = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.enc(x)\n",
    "        if not self.is_vae:\n",
    "            return out\n",
    "        mu, logvar = torch.chunk(out, 2, dim=-1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.dec(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5a756",
   "metadata": {},
   "source": [
    "# 2 Loss Functions\n",
    "\n",
    "## 2.1 Autoencoder\n",
    "\n",
    "It is quite simple to train an autoencoder. The simplest thing one can do is minimize the objective: $$ L_{\\text{AE}}((\\theta, \\phi); x) = \\|x - D_{\\theta}(E_{\\phi}(x))\\|^2.$$ The norm can be L1 (Mean Absolute Error) or L2 (Mean Squared Error). Try them both, see what happens! We are effectively trying to find a compressed \"bottleneck\" representation $z \\in \\R^{d_{\\text{latent}}}$ of $x \\in \\R^{d_{\\text{data}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27556790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ae(x, x_recon, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction loss for a standard Autoencoder.\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        # Default: Mean Squared Error\n",
    "        return torch.mean(...) # TODO\n",
    "    elif loss_type == 'mae':\n",
    "        # Mean Absolute Error\n",
    "        return torch.mean(...) # TODO: You can use torch.abs()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebba75e",
   "metadata": {},
   "source": [
    "## 2.2 Variational Autoencoder (VAE)\n",
    "\n",
    "A standard AE does not define a known distribution for $z$, making it impossible to sample new data! The VAE solves this by forcing the latent space to follow a prior $p(z)$, typically a Gaussian $N(0,I)$. We will refer to the probability distribution of the Encoder as $q_{\\phi}(z|x)$ and Decoder as $p_{\\theta}(x|z)$.\n",
    "\n",
    "Following Lecture 3 Slide 12, it was shown that:\n",
    "$$\n",
    "L_{\\text{VAE}}((\\theta, \\phi); x)\n",
    "=\n",
    "\\underbrace{\n",
    "\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}\\!\\left[\\|x - D_{\\theta}(z)\\|^2\\right]\n",
    "}_{\\text{reconstruction loss}}\n",
    "+\n",
    "\\underbrace{\n",
    "\\mathrm{KL}\\!\\left(q_{\\phi}(z|x)\\,\\|\\,p(z)\\right)\n",
    "}_{\\text{prior matching loss}}.\n",
    "$$\n",
    "\n",
    "The expectation is obviously approximated using Monte-Carlo.\n",
    "\n",
    "If $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^2)$, the KL term has the closed-form solution:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\bigl(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\bigr)\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\sum_{j=1}^{d_{\\text{latent}}}\n",
    "\\left(\n",
    "\\sigma_j^2\n",
    "+\n",
    "\\mu_j^2\n",
    "-\n",
    "1\n",
    "-\n",
    "\\log \\sigma_j^2\n",
    "\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245baa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_vae(x, x_recon, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE objective (Negative ELBO).\n",
    "    Based on Lecture 3, Slide 12.\n",
    "    \"\"\"\n",
    "    # 1. Reconstruction Loss (Standard MSE)\n",
    "    recon_loss = torch.mean(...) # TODO: Should be same as the one in loss_ae\n",
    "    \n",
    "    # 2. Prior-matching Loss (KL Divergence for Gaussians)\n",
    "    # This forces the latent space to follow N(0, I)\n",
    "    kl_loss = 0.5 * torch.mean(torch.sum(..., dim=1)) # TODO\n",
    "    \n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4dd245",
   "metadata": {},
   "source": [
    "### 2.2.1 Reparameterization Trick\n",
    "\n",
    "To allow backpropagation through stochastic nodes, we use the reparameterization: $$ z = \\mu + \\sigma \\odot \\epsilon $$ where $\\epsilon \\sim \\mathcal{N}(0, I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe96ed",
   "metadata": {},
   "source": [
    "## 2.3 Importance-Weighted VAE (IW-VAE)\n",
    "\n",
    "We will now implement the **Importance-Weighted Autoencoder (IWAE)** loss. This model addresses a key limitation of the standard VAE: the *looseness* of the Evidence Lower Bound (ELBO).\n",
    "\n",
    "In a standard VAE, we maximize the ELBO for a single data point $x$:\n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}(x)\n",
    "\\ge\n",
    "\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}\n",
    "\\left[\n",
    "\\log\n",
    "\\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The IWAE improves this by using $K$ samples from the approximate posterior\n",
    "to create a tighter lower bound:\n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}(x)\n",
    "\\ge\n",
    "\\mathbb{E}_{z_1, \\ldots, z_K \\sim q_{\\phi}(z \\mid x)}\n",
    "\\left[\n",
    "\\log\n",
    "\\frac{1}{K}\n",
    "\\sum_{k=1}^{K}\n",
    "\\frac{p_{\\theta}(x, z_k)}{q_{\\phi}(z_k \\mid x)}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "### Why use IWAE?\n",
    "\n",
    "- **Tighter Bound:** As $K \\to \\infty$, the IWAE bound approaches the true\n",
    "  log-likelihood $\\log p_{\\theta}(x)$.\n",
    "\n",
    "- **Expressive Posteriors:** IWAE effectively increases the expressiveness of\n",
    "  the variational posterior without changing the model architecture.\n",
    "\n",
    "- **Better Likelihood Estimation:** It provides a more accurate estimate for\n",
    "  evaluating new data.\n",
    "\n",
    "\n",
    "Please refer to this very well-written blog post for more info on IW-VAE (https://borea17.github.io/paper_summaries/iwae/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_iwae(x, mu, logvar, decoder, K=5):\n",
    "    \"\"\"\n",
    "    Computes the IWAE objective as a tighter lower bound on log p(x).\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    log_weights = []\n",
    "\n",
    "    # Sample K times for each input in the batch \n",
    "    for _ in range(K):\n",
    "        # Sample z from the approximate posterior using reparameterize()\n",
    "        z_k = reparameterize(mu, logvar) # [batch_size, latent_dim]\n",
    "        x_recon_k = decoder(z_k)\n",
    "        \n",
    "        # log p(x|z) - Log-likelihood of the data given the sample\n",
    "        # Assuming Gaussian reconstruction with fixed variance\n",
    "        log_p_x_z = -torch.sum((x - x_recon_k)**2, dim=1)\n",
    "        \n",
    "        # log p(z) - Log-prior probability of the sample\n",
    "        log_p_z = -0.5 * torch.sum(z_k**2, dim=1)\n",
    "        \n",
    "        # log q(z|x) - Log-probability of the sample under the approximate posterior\n",
    "        log_q_z_x = -0.5 * torch.sum(..., dim=1) # TODO: Try to remember what the Gaussian density function is.\n",
    "        \n",
    "        # Compute the importance weight for this sample \n",
    "        # log( p(x,z) / q(z|x) ) = log p(x|z) + log p(z) - log q(z|x)\n",
    "        log_weights.append(log_p_x_z + log_p_z - log_q_z_x)\n",
    "        \n",
    "    # Stack samples to shape [K, batch_size]\n",
    "    log_w_matrix = torch.stack(log_weights)\n",
    "    \n",
    "    # Apply the Log-Sum-Exp trick to average over K samples \n",
    "    # We negate because we minimize the negative log-likelihood estimate\n",
    "    iwae_loss = -(torch.logsumexp(log_w_matrix, dim=0) - torch.log(torch.tensor(K, dtype=torch.float))).mean()\n",
    "    return iwae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b6c68",
   "metadata": {},
   "source": [
    "## Putting them together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model_type, x, x_recon, mu=None, logvar=None, decoder=None, K=5, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Consolidated loss function for AE, VAE, and IWAE.\n",
    "    \"\"\"\n",
    "    if model_type == \"AE\":\n",
    "        return loss_ae(x, x_recon, loss_type=loss_type)\n",
    "    \n",
    "    elif model_type == \"VAE\":\n",
    "        if mu is None or logvar is None:\n",
    "            raise ValueError(\"VAE requires mu and logvar for the KL term.\")\n",
    "        return loss_vae(x, x_recon, mu, logvar)\n",
    "    \n",
    "    elif model_type == \"IWAE\":\n",
    "        if mu is None or logvar is None or decoder is None:\n",
    "            raise ValueError(\"IWAE requires mu, logvar, and the decoder network.\")\n",
    "        return loss_iwae(x, mu, logvar, decoder, K=K)    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef983408",
   "metadata": {},
   "source": [
    "# 3 Data Loading: Synthetic High-Dimensional GMM\n",
    "\n",
    "We will generate a synthetic high-dimensional dataset where the data lies on a manifold that can be compressed. A classic choice for this is a Gaussian Mixture Model (GMM) in high dimensions, which provides distinct \"classes\" for visualization. We will generate data in $\\R^{100}$ clustered around 5 centers. Although the data is 100D, the underlying structure is simple enough for a 2D or 3D latent space to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_data(n_samples=5000, input_dim=100, n_classes=5):\n",
    "    \"\"\"\n",
    "    Generates high-dimensional synthetic data with distinct clusters.\n",
    "    \"\"\"\n",
    "    # Create clusters in high-dimensional space\n",
    "    data, labels = make_blobs(\n",
    "        n_samples=n_samples, \n",
    "        n_features=input_dim, \n",
    "        centers=n_classes, \n",
    "        cluster_std=1.5, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize data\n",
    "    data = (data - data.mean()) / data.std()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x_train = torch.FloatTensor(data)\n",
    "    y_train = torch.LongTensor(labels)\n",
    "    \n",
    "    dataset = TensorDataset(x_train, y_train)\n",
    "    return DataLoader(dataset, batch_size=64, shuffle=True), input_dim\n",
    "\n",
    "# Initialize data\n",
    "train_loader, input_dim = load_synthetic_data()\n",
    "print(f\"Loaded {len(train_loader.dataset)} samples with input dimension {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714a94f",
   "metadata": {},
   "source": [
    "# 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, model_type='VAE', epochs=20, lr=1e-3, K=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs + 1):\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if model_type == 'AE':\n",
    "                z = model.encode(x)\n",
    "                x_recon = model.decode(z)\n",
    "                loss = compute_loss('AE', x, x_recon)\n",
    "            \n",
    "            elif model_type == 'VAE':\n",
    "                mu, logvar = model.encode(x)\n",
    "                z = reparameterize(mu, logvar)\n",
    "                x_recon = model.decode(z)\n",
    "                loss = compute_loss('VAE', x, x_recon, mu=mu, logvar=logvar)\n",
    "                \n",
    "            elif model_type == 'IWAE':\n",
    "                # IWAE requires mu and logvar to sample internally within loss_iwae\n",
    "                mu, logvar = model.encode(x)\n",
    "                # We pass the decoder because IWAE needs to reconstruct K times\n",
    "                loss = compute_loss('IWAE', x, None, mu=mu, logvar=logvar, \n",
    "                                    decoder=model.dec, K=K)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if epoch % 5 == 0 or epoch == 0:\n",
    "            print(f\"[{model_type}] Epoch {epoch}/{epochs}, Average Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Setup experiment\n",
    "latent_dim = 2 \n",
    "\n",
    "# 1. Train Autoencoder\n",
    "print(\"--- Training Autoencoder ---\")\n",
    "ae_model = GenerativeModel(input_dim, latent_dim, is_vae=False)\n",
    "train_model(ae_model, train_loader, model_type='AE', epochs=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 2. Train Variational Autoencoder\n",
    "print(\"--- Training Variational Autoencoder ---\")\n",
    "vae_model = GenerativeModel(input_dim, latent_dim, is_vae=True)\n",
    "train_model(vae_model, train_loader, model_type='VAE', epochs=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 3. Train Importance-Weighted Autoencoder\n",
    "print(\"--- Training Importance-Weighted Autoencoder ---\")\n",
    "# IWAE uses the same stochastic encoder architecture as VAE\n",
    "iwae_model = GenerativeModel(input_dim, latent_dim, is_vae=True)\n",
    "train_model(iwae_model, train_loader, model_type='IWAE', epochs=20, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38140c48",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "\n",
    "What do you observe about the losses? Is one worse than the other? Why is the scale of IWAE and VAE different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef120253",
   "metadata": {},
   "source": [
    "# 5 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_three_comparison(ae_model, vae_model, iwae_model, train_loader):\n",
    "    models = [('AE', ae_model), ('VAE', vae_model), ('IWAE', iwae_model)]\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    for i, (model_type, model) in enumerate(models):\n",
    "        model.eval()\n",
    "        latents, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in train_loader:\n",
    "                out = model.encode(x)\n",
    "                z = out[0] if isinstance(out, tuple) else out # Use mu for VAE/IWAE\n",
    "                latents.append(z)\n",
    "                labels.append(y)\n",
    "\n",
    "        latents = torch.cat(latents, dim=0).numpy()\n",
    "        labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=15)\n",
    "        plt.title(f'{model_type} Latent Space')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        #if model_type != 'AE': plt.xlim(-4, 4); plt.ylim(-4, 4)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_three_comparison(ae_model, vae_model, iwae_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de13dd6",
   "metadata": {},
   "source": [
    "### Discusion:\n",
    "\n",
    "Observe the difference in scales. Why do you think VAE is collapsing so close to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5c1ee",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Let us repeat that but with images. We will use the famed (but simple) MNIST dataset. Running the code below, it should load MNIST data and save into the same folder under \"data/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a79106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: torch.flatten(x)) # Flatten 28x28 to 784\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, 784\n",
    "\n",
    "# Initialize MNIST\n",
    "mnist_loader, mnist_dim = load_mnist_data()\n",
    "print(f\"Loaded MNIST: {len(mnist_loader.dataset)} images, Input Dimension: {mnist_dim}\")\n",
    "\n",
    "def visualize_mnist_samples(loader, n_samples=16):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Reshape flattened 784 back to (1, 28, 28) for visualization\n",
    "    images = images.view(-1, 1, 28, 28)\n",
    "    \n",
    "    # Create a grid\n",
    "    grid = torchvision.utils.make_grid(images[:n_samples], nrow=4)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"MNIST Training Samples\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_mnist_samples(mnist_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_reconstructions(models_list, loader, n_samples=10):\n",
    "    \"\"\"\n",
    "    Visualizes original images followed by reconstructions from each model side-by-side.\n",
    "    \"\"\"\n",
    "    for _, model in models_list:\n",
    "        model.eval()\n",
    "        \n",
    "    # Get a batch of samples\n",
    "    data_iter = iter(loader)\n",
    "    images, _ = next(data_iter)\n",
    "    images = images[:n_samples]\n",
    "    \n",
    "    # Create a grid for (Original + n_models) rows\n",
    "    plt.figure(figsize=(n_samples * 1.5, (len(models_list) + 1) * 1.5))\n",
    "    \n",
    "    # 1. Plot Original Inputs\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(len(models_list) + 1, n_samples, i + 1)\n",
    "        # Reshape flattened 784 back to 28x28\n",
    "        plt.imshow(images[i].view(28, 28).cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(\"Original Data\", loc='left', fontsize=12, fontweight='bold')\n",
    "            \n",
    "    # 2. Plot Reconstructions for each model row by row\n",
    "    for m_idx, (name, model) in enumerate(models_list):\n",
    "        with torch.no_grad():\n",
    "            # Pass through encoder\n",
    "            out = model.encode(images)\n",
    "            # Use mu for probabilistic models (VAE/IWAE) or the direct z for AE\n",
    "            z = out[0] if isinstance(out, tuple) else out \n",
    "            \n",
    "            # Decode the latent back to image space\n",
    "            recons = model.decode(z)\n",
    "            recons = recons.view(-1, 28, 28).cpu().numpy()\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                plt.subplot(len(models_list) + 1, n_samples, (m_idx + 1) * n_samples + i + 1)\n",
    "                plt.imshow(recons[i], cmap='gray')\n",
    "                plt.axis('off')\n",
    "                if i == 0:\n",
    "                    plt.title(f\"{name} Recon\", loc='left', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08951e",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This will take about 4 minutes total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "# 1. Train MNIST Autoencoder (AE)\n",
    "print(\"--- Training MNIST Autoencoder ---\")\n",
    "ae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=False)\n",
    "train_model(ae_mnist, mnist_loader, model_type='AE', epochs=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 2. Train MNIST Variational Autoencoder (VAE)\n",
    "print(\"--- Training MNIST Variational Autoencoder ---\")\n",
    "vae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=True)\n",
    "train_model(vae_mnist, mnist_loader, model_type='VAE', epochs=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 3. Train MNIST Importance-Weighted Autoencoder (IWAE)\n",
    "print(\"--- Training MNIST IWAE ---\")\n",
    "iwae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=True)\n",
    "train_model(iwae_mnist, mnist_loader, model_type='IWAE', epochs=15, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a257fb7",
   "metadata": {},
   "source": [
    "## Visualize in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_three_comparison(ae_mnist, vae_mnist, iwae_mnist, mnist_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    ('AE', ae_mnist),\n",
    "    ('VAE', vae_mnist),\n",
    "    ('IWAE', iwae_mnist)\n",
    "]\n",
    "\n",
    "# Run visualization on the MNIST loader\n",
    "visualize_all_reconstructions(models_to_compare, mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569f7fd",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "Do we observe the same issue with VAE? How does it translate when reconstructing a new image? What do you observe about the difference between the AE and IWAE latent spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93838992",
   "metadata": {},
   "source": [
    "## Increase latent dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "\n",
    "# 1. Train MNIST Autoencoder (AE)\n",
    "print(\"--- Training MNIST Autoencoder ---\")\n",
    "ae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=False)\n",
    "train_model(ae_mnist, mnist_loader, model_type='AE', epochs=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 2. Train MNIST Variational Autoencoder (VAE)\n",
    "print(\"--- Training MNIST Variational Autoencoder ---\")\n",
    "vae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=True)\n",
    "train_model(vae_mnist, mnist_loader, model_type='VAE', epochs=15)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 3. Train MNIST Importance-Weighted Autoencoder (IWAE)\n",
    "print(\"--- Training MNIST IWAE ---\")\n",
    "iwae_mnist = GenerativeModel(mnist_dim, latent_dim, is_vae=True)\n",
    "train_model(iwae_mnist, mnist_loader, model_type='IWAE', epochs=15, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c387a7b",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "Now that we increased latent dimension, do you notice any differences in the learning? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfc5bd",
   "metadata": {},
   "source": [
    "## Visualize in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_three_comparison_3d(ae_model, vae_model, iwae_model, train_loader):\n",
    "    \"\"\"\n",
    "    Plots the 3D latent space of AE, VAE, and IWAE side-by-side.\n",
    "    \"\"\"\n",
    "    models = [('AE', ae_model), ('VAE', vae_model), ('IWAE', iwae_model)]\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    \n",
    "    for i, (model_type, model) in enumerate(models):\n",
    "        model.eval()\n",
    "        latents, labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in train_loader:\n",
    "                out = model.encode(x)\n",
    "                # Use mu for VAE/IWAE, or the direct bottleneck for AE\n",
    "                z = out[0] if isinstance(out, tuple) else out\n",
    "                latents.append(z)\n",
    "                labels.append(y)\n",
    "\n",
    "        latents = torch.cat(latents, dim=0).numpy()\n",
    "        labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "        ax = fig.add_subplot(1, 3, i + 1, projection='3d')\n",
    "        scatter = ax.scatter(latents[:, 0], latents[:, 1], latents[:, 2], \n",
    "                             c=labels, cmap='tab10', alpha=0.5, s=5)\n",
    "        \n",
    "        ax.set_title(f'{model_type} 3D Latent Space')\n",
    "        ax.set_xlabel('z1')\n",
    "        ax.set_ylabel('z2')\n",
    "        ax.set_zlabel('z3')\n",
    "        \n",
    "        # Consistent scaling for probabilistic models to see the Gaussian prior match\n",
    "        #if model_type != 'AE':\n",
    "            #ax.set_xlim(-4, 4); ax.set_ylim(-4, 4); ax.set_zlim(-4, 4)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_three_comparison_3d(ae_mnist, vae_mnist, iwae_mnist, mnist_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    ('AE', ae_mnist),\n",
    "    ('VAE', vae_mnist),\n",
    "    ('IWAE', iwae_mnist)\n",
    "]\n",
    "\n",
    "# Run visualization on the MNIST loader\n",
    "visualize_all_reconstructions(models_to_compare, mnist_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
